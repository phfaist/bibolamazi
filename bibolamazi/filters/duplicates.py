################################################################################
#                                                                              #
#   This file is part of the Bibolamazi Project.                               #
#   Copyright (C) 2013-2018 by Philippe Faist                                  #
#   philippe.faist@bluewin.ch                                                  #
#                                                                              #
#   Bibolamazi is free software: you can redistribute it and/or modify         #
#   it under the terms of the GNU General Public License as published by       #
#   the Free Software Foundation, either version 3 of the License, or          #
#   (at your option) any later version.                                        #
#                                                                              #
#   Bibolamazi is distributed in the hope that it will be useful,              #
#   but WITHOUT ANY WARRANTY; without even the implied warranty of             #
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the              #
#   GNU General Public License for more details.                               #
#                                                                              #
#   You should have received a copy of the GNU General Public License          #
#   along with Bibolamazi.  If not, see <http://www.gnu.org/licenses/>.        #
#                                                                              #
################################################################################

# Py2/Py3 support
from __future__ import unicode_literals, print_function
from past.builtins import basestring
from future.utils import python_2_unicode_compatible, iteritems
from builtins import range
from builtins import str as unicodestr


import os
import os.path
import re
import codecs
import unicodedata
import string
import textwrap
import copy
import logging

from pybtex.database import BibliographyData, Entry
from pybtex.utils import OrderedCaseInsensitiveDict
import pybtex.textutils

from pylatexenc import latex2text

from bibolamazi.core.bibfilter import BibFilter, BibFilterError
from bibolamazi.core.bibfilter.argtypes import CommaStrList
from bibolamazi.core import butils
from bibolamazi.core import bibusercache
from bibolamazi.core.bibusercache import tokencheckers

from .util import arxivutil
from .util import auxfile

logger = logging.getLogger(__name__)

### DON'T CHANGE THIS STRING. IT IS THE STRING THAT IS SEARCHED FOR IN AN EXISTING
### DUPFILE TO PREVENT OVERWRITING OF WRONG FILES.
### SERIOUSLY. IT DOESN'T NEED TO CHANGE.
BIBALIAS_WARNING_HEADER = """\
% NOTE: THIS FILE WAS AUTOMATICALLY GENERATED BY bibolamazi SCRIPT!
%       ANY CHANGES WILL BE LOST!
"""

BIBALIAS_HEADER = r"""%
####BIBALIAS_WARNING_HEADER####
%
% File automatically generated by bibolamazi's `duplicates` filter.
%
% You should include this file in your main LaTeX file with the command
%
%   \input{####DUP_FILE_NAME####}
%
% in your document preamble.
%

""".replace('####BIBALIAS_WARNING_HEADER####\n', BIBALIAS_WARNING_HEADER)

BIBALIAS_LATEX_DEFINITIONS = r"""

%
% The following will define the command \bibalias{<alias>}{<source>}, which will make
% the command \cite[..]{<alias>} the same as doing \cite[..]{<source>}.
%
% This code has been copied and adapted from
%    http://tex.stackexchange.com/questions/37233/
%

\makeatletter
% \bibalias{<alias>}{<source>} makes \cite{<alias>} equivalent to \cite{<source>}
\newcommand\bibalias[2]{%
  \@namedef{bibali@#1}{#2}%
}


% Code taken from from etextools.sty, apparently originally borrowed from environ.sty
\newcommand\biba@deblank[1]{\romannumeral\biba@deblank@#1/ /} % dark magic. stay away
\long\def\biba@deblank@#1 /{\biba@deblank@i#1/}
\long\def\biba@deblank@i#1/#2{\z@#1}


\newtoks\biba@toks
\let\bibalias@oldcite\cite
\def\cite{%
  \@ifnextchar[{%
    \biba@cite@optarg%
  }{%
    \biba@cite{}%
  }%
}
\newcommand\biba@cite@optarg[2][]{%
  \biba@cite{[#1]}{#2}%
}
\newcommand\biba@cite[2]{%
  \biba@toks{\bibalias@oldcite#1}%
  \def\biba@comma{}%
  \def\biba@all{}%
  \def\biba@argkeys{}%
  \@for\biba@one@:=#2\do{%
    \edef\biba@one{\expandafter\@firstofone\biba@one@\@empty}%
    \edef\biba@one{\expandafter\biba@deblank\expandafter{\biba@one}}% remove whitespace
    \edef\biba@argkeys{\biba@argkeys\biba@comma\biba@one}%
    \@ifundefined{bibali@\biba@one}{%
      \edef\biba@all{\biba@all\biba@comma\biba@one}%
    }{%
      \PackageInfo{bibalias}{%
        Replacing citation `\biba@one' with `\@nameuse{bibali@\biba@one}'
      }%
      \edef\biba@all{\biba@all\biba@comma\@nameuse{bibali@\biba@one}}%
    }%
    \def\biba@comma{,}%
  }%
  %
  % However, still write in the .aux file a dummy \citation{...} command, so that
  % filters.util.auxfile will still catch those used citations
  %
  \immediate\write\@auxout{\noexpand\bgroup\noexpand\renewcommand\noexpand\citation[1]{}\noexpand\citation{\biba@argkeys}\noexpand\egroup}%
  %
  % Now, produce the \cite command with the original keys instead of the aliases
  %
  \edef\biba@tmp{\the\biba@toks{\biba@all}}%
  \biba@tmp%
}
\makeatother


%
% Now, declare all the alias keys.
%

"""



DUPL_WARN_TOP = """

    DUPLICATE ENTRIES WARNING
    -------------------------

"""

DUPL_WARN_ENTRY = """\
    %(alias)-20s   ** duplicate of **  %(orig)s
"""
DUPL_WARN_ENTRY_MIDCOL=4+20+3+len('** duplicate of **  ') + 2; # column no. to start text of "original"
DUPL_WARN_ENTRY_BEGCOL=6; # column no. to start text of "alias"
DUPL_WARN_ENTRY_COLSEP = 4
DUPL_WARN_ENTRY_COLWIDTH = DUPL_WARN_ENTRY_MIDCOL - DUPL_WARN_ENTRY_COLSEP - DUPL_WARN_ENTRY_BEGCOL

DUPL_WARN_BOTTOM = """\
    There were %(num_dupl)d duplicates.
    -------------------------

"""



# --------------------------------------------------


# some utilities


# these words will get stripped from journal names when forming abbreviations
BORING_WORDS = (
    "a",
    "of",
    "the",
    "in",
    "on",
    "and",
    "its",
    "de",
    "et",
    "der",
    "und",
    )


def normstr(x, lower=True):
    if not isinstance(x, unicodestr):
        x = unicodestr(x.decode('utf-8'))

    x2 = unicodedata.normalize('NFKD', x).strip()
    if lower:
        x2 = x2.lower()
    # remove any unicode compositions (accents, etc.)
    x2 = re.sub(b'[^\\x00-\\x7f]', b'', x2.encode('utf-8')).decode('utf-8')
    ## additionally, remove any special LaTeX chars which may be written differently.
    #x2 = re.sub(r'\\([a-zA-Z]+|.)', '', x2)
    x2 = re.sub(r'''[\{\}\|\.\+\?\*\,\'\"\\]''', '', x2)
    x2 = re.sub(r'-+', '-', x2)
    #logger.longdebug("Normalized string: %r -> %r", x, x2)
    return x2

def getlast(pers, lower=True):
    # join last names
    last = normstr(unicodestr(butils.latex_to_text(" ".join(pers.prelast_names+pers.last_names)).split()[-1]),
                   lower=lower)
    initial = re.sub('[^a-z]', '',
                     normstr(u"".join([pybtex.textutils.abbreviate(x) for x in pers.first_names]),
                             lower=lower),
                     flags=re.IGNORECASE)[0:1] # only first initial [a-z]
    return (last, initial)

def fmtjournal(x):
    if not isinstance(x, unicodestr):
        x = unicodestr(x.decode('utf-8'))
        
    x2 = normstr(x, lower=False)

    # drop "a", "the", "on", "of"
    # -- HACK to keep the `A' in ``Physics Review A'': Trick it into thinking as
    #    ``Physics Review Aaaa'', and then only capital letters are kept anyway
    x2 = re.sub(r'\s+(a)\s*([.:,]|$)', ' Aaaaaa', x2, flags=re.IGNORECASE)
    # --
    x2 = re.sub(r'\b(' + r'|'.join(BORING_WORDS) + r')\b', '', x2, flags=re.IGNORECASE)
    #logger.longdebug('fmtjournal TEMP: %r', x2)

    x2 = re.sub(r'\b([a-z])', lambda m: m.group().capitalize(), x2)
    x2 = re.sub(r'[^A-Z]', '', x2)
    #logger.longdebug('fmtjournal TEMP final: %r', x2)
    return x2


def sanitize_doi(doi):
    if doi is None:
        return None
    return re.sub(r'^DOI\s*:?\s*', '', doi.strip(), flags=re.IGNORECASE)

# -------------

# utility for comparing months:
_mon_list = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']
_mon = dict(zip(_mon_list,range(1,12)))

def normalize_month(x):
    m = x.strip().lower()[:3]
    if m in _mon:
        return _mon[m]
    return -1







# --------------------------------------------------








HELP_AUTHOR = u"""\
Duplicates filter by Philippe Faist, (C) 2013-2018, GPL 3+
"""

HELP_DESC = u"""\
Produces LaTeX rules to make duplicate entries aliases of one another.
"""

HELP_TEXT = r"""
This filter works by writing a LaTeX file (the ``dupfile''), which contains the
commands needed to define the bibtex aliases. You may then use the LaTeX `\cite'
command with either key; duplicate entries are merged and a single entry will be
produced in the bibliography.

The dupfile, which contains the necessary definitions to make keys aliases of
their duplicate entries, is specifed via the `--dupfile' option as
`--dupfile=yourdupfile.tex' or with `-sDupfile=yourdupfile.tex'.

In your main LaTeX document, you need to add the following command in the
preamble:

  \input{yourdupfile.tex}

where of couse yourdupfile.tex is the file that you specified to this filter.

Alternatively, if you just set the warn flag on, then a duplicate file is not
created (unless the dupfile option is also given), and a warning is displayed
for each duplicate found. The duplicates *ARE* however still removed and merged
in the final bibfile.

Duplicates are detected based on several heuristics and quite extensive
testing. Recently I've been pretty much satisfied with the results, but there is
no guarantee that it won't miss some matches sometimes. The algorithm is
designed to prevent as much as possible any false matches, so that shouldn't
normally happen, but again there's no guarantee. Also, if two entries refer to
different versions of a paper where one is on the arXiv and the other is
published, they are NOT considered as duplicates.

If you set -dKeepOnlyUsed, then while sorting out the duplicates, we only keep
those entries that are referred to from within the document. For this to work,
you need to have called latex/pdflatex on your document first. The document is
assumed to have the same base name as your bibolamazi file, for instance, if
your bibolamazi file is called "myfile.bibolamazi.bib", then we assume that we
can read the citations in the aux file produced by LaTeX called "myfile.aux".
If this is not the case, you may specify the base name of your LaTeX document
using the option "-sJobname=myfile". Note -dKeepOnlyUsed only works if you are
merging duplicates (i.e., you didn't set -dMergeDuplicates=False).

The dupfile will be by default self-contained, i.e. will contain all the
definitions necessary so that you can use the different cite keys transparently
with the `\cite` LaTeX command. However the implementation of the `\cite'
command is a bit minimal. For example, no spaces are allowed between its
arguments, and other commands such as `\citep' are not supported.

If you specify the `-dCustomBibalias' option, then the dupfile will only contain
a list of duplicate definitions of the form

    \bibalias{<alias>}{<original>}

without any definition of the `\bibalias' command itself. It is thus up to the
user to provide a usable `\bibalias' command, before the `\input{<dupfile>}'
invocation. Use this option to get most flexibly on how you want to treat your
aliases, but this will require a little more work from your side.
"""


# ------------------------------------------------


class DuplicatesEntryInfoCacheAccessor(bibusercache.BibUserCacheAccessor):
    def __init__(self, **kwargs):
        super(DuplicatesEntryInfoCacheAccessor, self).__init__(
            cache_name='duplicates_entryinfo',
            **kwargs
            )


    def initialize(self, cache_obj, **kwargs):
        #
        # Make sure we set up cache invalidation properly, to ensure that if a
        # user modifies a falsely picked-up duplicate, that the cache is
        # updated!
        #

        cache_entries_validator = tokencheckers.EntryFieldsTokenChecker(
            self.bibolamaziFile().bibliographyData(),
            store_type=True,
            store_persons=['author'],
            fields=list(set(
                # from arxivInfo
                arxivutil.arxivinfo_from_bibtex_fields +
                [
                    'note',
                    'journal',
                    'title',
                    ])),
            )

        self.cacheDic()['entries'].set_validation(cache_entries_validator)


    def prepare_entry_cache(self, key, a, arxivaccess):

        entriescache = self.cacheDic()['entries']

        if key in entriescache and entriescache[key]:
            return

        cache_a = entriescache[key]

        cache_a['pers'] = [ getlast(pers) for pers in a.persons.get('author',[]) ]

        cache_a['arxivinfo'] = arxivaccess.getArXivInfo(a.key)

        note = a.fields.get('note', '')
        cache_a['note_cleaned'] = (arxivutil.stripArXivInfoInNote(note) if note else "")
        
        cache_a['j_abbrev'] = fmtjournal(a.fields.get('journal', ''))

        def cleantitle(title):
            title = unicodedata.normalize('NFKD', unicodestr(butils.latex_to_text(title).lower()))
            # remove any unicode compositions (accents, etc.)
            title = re.sub(b'[^\\x00-\\x7f]', b'', title.encode('utf-8')).decode('utf-8')
            # remove any unusual characters
            title = re.sub(r'[^a-zA-Z0-9 ]', '', title)
            # remove any inline math
            title = re.sub(r'$[^$]+$', '', title)
            # clean up whitespace
            title = re.sub(r'\s+', ' ', title)
            return title.strip()

        cache_a['title_clean'] = cleantitle(a.fields.get('title', ''))


    def get_entry_cache(self, key):
        return self.cacheDic()['entries'][key]





    



# ------------------------------------------------



class DuplicatesFilter(BibFilter):

    helpauthor = HELP_AUTHOR
    helpdescription = HELP_DESC
    helptext = HELP_TEXT


    def __init__(self, dupfile=None, merge_duplicates=True,
                 ensure_conflict_keys_are_duplicates=True,
                 warn=False, custom_bibalias=False,
                 keep_only_used=None, jobname=None,
                 keep_only_used_in_jobname=None, jobname_search_dirs=None,
                 ignore_fields_warning=None,
                 *args):
        r"""DuplicatesFilter constructor.

        *dupfile: the name of a file to write latex code for defining duplicates to. This file
               will be overwritten!!
        *merge_duplicates(bool): Whether to actually merge the duplicates in the bibliography data
               or not. True by default.
        *ensure_conflict_keys_are_duplicates(bool): If true (the default), then keys of the form
               "xxxxxx.conflictkey.N" are verified to indeed be duplicates of the corresponding
               "xxxxxx" entry. These conflict key entries are created automatically when two
               bibtex sources have entries with the same key.
        *warn(bool): if this flag is set, a warning is issued for every duplicate entry found
               in the database.
        *custom_bibalias(bool): if set to TRUE, then no latex definitions will be generated
               in the file given in `dupfile', and will rely on a user-defined implementation
               of `\bibalias`.
        *keep_only_used(bool): if TRUE, then only keep entries which are referenced in the
               LaTeX file.  Use the --jobname option to specify which LaTeX jobname to look at.
               Note: This option has no effect if the `merge_duplicates' option is off.
        *jobname: If --keep-only-used is specified, then look for used citations in the LaTeX
               file with this base name.  See also the only_used filter. The corresponding AUX
               file is searched for and analyzed. If --jobname is not specified, then the
               LaTeX file name is guessed from the bibolamazi file name, as for the only_used
               filter.
        *ignore_fields_warning(CommaStrList): Normally, warnings are issued if some fields
               differ between entries that are detected as duplicates.  List some fields here
               for which you don't care. (By default, this is a reasonable collection of
               fields which normally don't matter, e.g., 'file'.)
        *keep_only_used_in_jobname: OBSOLTE
        *jobname_search_dirs(CommaStrList): (use with -sJobname) search for the
               AUX file in the given directories, as for the only_used filter.
        """

        BibFilter.__init__(self)

        self.dupfile = dupfile

        self.merge_duplicates = butils.getbool(merge_duplicates)

        self.warn = butils.getbool(warn)
        
        self.custom_bibalias = butils.getbool(custom_bibalias)
        
        self.ensure_conflict_keys_are_duplicates = butils.getbool(ensure_conflict_keys_are_duplicates)

        if len(args) == 1:
            if self.dupfile is None:
                self.dupfile = args[0]
            else:
                raise BibFilterError("duplicates",
                                     "Repeated values given for dupfile: one as an option (`%s'), "
                                     "the other as a positional argument (`%s')"%(self.dupfile, args[0]))
        elif len(args) != 0:
            raise BibFilterError("duplicates",
                                 "Received unexpected positional arguments (at most one expected, "
                                 "the dupfile name): [%s]"%(",".join(["%s"%(x) for x in args])))

        self.keep_only_used = False
        self.jobname = None
        
        if keep_only_used_in_jobname:
            logger.warning("duplicates filter: the -sKeepOnlyUsedInJobname=<jobname> option is now obsolete. "
                           "Please replace this option by '-dKeepOnlyUsed -sJobname=<jobname>'")
            self.keep_only_used = True
            self.jobname = keep_only_used_in_jobname

        if keep_only_used is not None:
            if keep_only_used_in_jobname:
                logger.warning("duplicates filter: options -sKeepOnlyUsedInJobname=<jobname> and "
                               " -dKeepOnlyUsed may not be specified simultaneously")
            self.keep_only_used = keep_only_used
            self.jobname = jobname

        if jobname_search_dirs is not None:
            jobname_search_dirs = CommaStrList(jobname_search_dirs)
        self.jobname_search_dirs = jobname_search_dirs

        if ignore_fields_warning:
            self.ignore_fields_warning = [ x.lower() for x in CommaStrList(ignore_fields_warning)]
        else:
            self.ignore_fields_warning = [
                # NOTE: must be all lowercase!
                
                # seriously, no latex document cares.
                'file', 'keywords', 'mendeley-tags',

                # these are already taken into account when detecting
                # duplicates
                'eprint', 'arxivid', 'archiveprefix',
            ]

        self.cache_entries_validator = None

        #if (not self.dupfile and not self.warn):
        #    logger.warning("bibolamazi duplicates filter: no action will be taken as neither -sDupfile or"+
        #                   " -dWarn are given!")

        logger.debug('duplicates: dupfile=%r, warn=%r' % (dupfile, warn))


    def getRunningMessage(self):
        if (self.dupfile):
            return ("Processing duplicate entries. Don't forget to insert `\\input{%s}' in "
                    "your LaTeX file!" %(self.dupfile) )
        return "Processing duplicate entries"
    

    def action(self):
        return BibFilter.BIB_FILTER_BIBOLAMAZIFILE


    def requested_cache_accessors(self):
        return [
            DuplicatesEntryInfoCacheAccessor,
            arxivutil.ArxivInfoCacheAccessor,
            arxivutil.ArxivFetchedAPIInfoCacheAccessor,
            ]


    def compare_entries(self, a, b, cache_a, cache_b):
        """
        Returns a tuple `(is_same, reason)` of a boolean and a string. The `reason`
        is a human-readable explanations of why the entries are considered the
        same or not.
        """

        # compare author list first

        logger.longdebug('Comparing entries %s and %s', a.key, b.key)

        apers = cache_a['pers']
        bpers = cache_b['pers']

        pending_pos_match_warning = []
        def pos_match():
            for w in pending_pos_match_warning:
                logger.warning(w)

        if (len(apers) != len(bpers)):
            return False, "Author list lengths %d and %d differ"%(len(apers), len(bpers))

        for k in range(len(apers)):
            (lasta, ina) = apers[k]
            (lastb, inb) = bpers[k]
            # use Levenshtein distance to detect possible typos or alternative spellings
            # (e.g. Koenig vs Konig). Allow approx. one such typo per 8 characters.
            lev_dist = levenshtein(lasta, lastb)
            if (lev_dist > (1+int(len(lasta)/8)) or (ina and inb and ina != inb)):
                return False, "Authors %r and %r differ"%((lasta, ina), (lastb, inb))
            if lev_dist > 0:
                pending_pos_match_warning.append(
                    "Duplicate entries {} and {} have possible typo in author name: \"{}\" vs \"{}\""
                    .format(a.key, b.key, str(a.persons.get('author',[])[k]), str(b.persons.get('author',[])[k]))
                )


        logger.longdebug("Author list matches! %r and %r ",apers,bpers)

        def compare_neq_fld(x, y, fld, filt=lambda x: x):
            xval = x.get(fld, None)
            yval = y.get(fld, None)
            try:
                xval = xval.strip()
            except AttributeError:
                pass
            try:
                yval = yval.strip()
            except AttributeError:
                pass

            return xval is not None and yval is not None and filt(xval) != filt(yval) 

        # authors are the same. check year
        if (compare_neq_fld(a.fields, b.fields, 'year')):
            logger.longdebug
            return False, "Years %r and %r differ"%(a.fields.get('year', None), b.fields.get('year', None))

        if (compare_neq_fld(a.fields, b.fields, 'month', filt=normalize_month)):
            return False, "Months %r and %r differ"%(a.fields.get('month', None), b.fields.get('month', None))

        doi_a = sanitize_doi(a.fields.get('doi'))
        doi_b = sanitize_doi(b.fields.get('doi'))
        if (doi_a and doi_b and doi_a != doi_b):
            return False, "DOI's %r and %r differ"%(doi_a, doi_b)
        if (doi_a and doi_a == doi_b):
            pos_match()
            return True, "DOI's %r and %r are the same"%(doi_a, doi_b)

        arxiv_a = cache_a['arxivinfo']
        arxiv_b = cache_b['arxivinfo']

        logger.longdebug("  arxiv_a=%r, arxiv_b=%r", arxiv_a, arxiv_b)
        
        if (arxiv_a and arxiv_b and
            'arxivid' in arxiv_a and 'arxivid' in arxiv_b and
            arxiv_a['arxivid'] != arxiv_b['arxivid']):
            return False, "arXiv IDS %r and %r differ"%(arxiv_a['arxivid'], arxiv_b['arxivid'])
        if (arxiv_a and arxiv_b and
            'arxivid' in arxiv_a and 'arxivid' in arxiv_b and
            arxiv_a['arxivid'] == arxiv_b['arxivid']):
            pos_match()
            return True, "arXiv IDS %r and %r same"%(arxiv_a['arxivid'], arxiv_b['arxivid'])


        # if they have different notes, then they're different entries
        note_cl_a = cache_a['note_cleaned']
        note_cl_b = cache_b['note_cleaned']
        if (note_cl_a and note_cl_b and note_cl_a != note_cl_b):
            return False, "Notes (cleaned up) %r and %r differ"%(note_cl_a, note_cl_b)

        # create abbreviations of the journals by keeping only the uppercase letters
        j_abbrev_a = cache_a['j_abbrev']
        j_abbrev_b = cache_b['j_abbrev']
        # don't require them to be equal, but just that they have good overlap... e.g. "PNAS" and "PNASUSA"
        # allow also one typo per approx. 4 chars
        if ( j_abbrev_a and j_abbrev_b and
             (levenshtein(j_abbrev_a[:len(j_abbrev_b)], j_abbrev_b[:len(j_abbrev_a)])
              > (1+int(min(len(j_abbrev_a),len(j_abbrev_b))/4))
             ) ):
            return False, "Journal (parsed & simplified) %r and %r differ"%(j_abbrev_a, j_abbrev_b)

        if ( compare_neq_fld(a.fields, b.fields, 'volume') ):
            return False, "Volumes %r and %r differ"%(a.fields.get('volume', None), b.fields.get('volume', None))

        if ( compare_neq_fld(a.fields, b.fields, 'number') ):
            return False, "Numbers %r and %r differ"%(a.fields.get('number', None), b.fields.get('number', None))

        titlea = cache_a['title_clean']
        titleb = cache_b['title_clean']

        if (titlea and titleb and titlea != titleb):
            return False, "Titles %r and %r differ"%(titlea, titleb)

        # ### Unreliable. Bad for arxiv entries and had some other bugs. (E.g. "123--5" vs "123--125" vs "123")
        #
        #if ( compare_neq_fld(a.fields, b.fields, 'pages') ):
        #    print("pages differ!")
        #    import pdb; pdb.set_trace()
        #    return False, "pages differ ....."

        #logger.longdebug("Entries %s and %s match.", a.key, b.key)

        # well at this point the publications are pretty much duplicates
        pos_match()
        return True, "Entries do not differ on the relevant fields"
        

    def update_entry_with_duplicate(self, origkey, origentry, duplkey, duplentry):
        """
        Merges definitions present in the duplicate entry, which are not present in the
        original. A very simple update-if-not-present mechanism is done, and no full-blown
        merge is performed. Simply entries which are not already present in the original
        are updated.
        """

        _mergefields = ['keywords','note','annote']
        _splitrx = re.compile(r'[;,]\s*') # no capture please!! because we use rx.split(...)

        for (fk, fval) in iteritems(duplentry.fields):
            # field not in original -- add it
            if fk not in origentry.fields or not origentry.fields[fk].strip():
                origentry.fields[fk] = fval
                continue

            origval = origentry.fields[fk]

            if fval.strip().lower() == origval.strip().lower():
                # fields agree on value -- no action needed
                continue

            # field is a list of keywords field -- simply merge all entries
            if fk in _mergefields:
                m = _splitrx.search(origval)
                if m:
                    splitchars = m.group()
                else:
                    splitchars = ','

                thelist = _splitrx.split(origval)
                thelistlower = [x.lower() for x in thelist]
                for newval in _splitrx.split(fval):
                    if newval and newval.lower() not in thelistlower:
                        thelist.append(newval)
                        thelistlower.append(newval.lower())
                origentry.fields[fk] = splitchars.join(thelist)
                continue

            # fields differ, but we don't know how to merge them. Warn the user
            # if it's a field that is not listed as an unimportant field
            if fk.lower() not in self.ignore_fields_warning:
                logger.warning("Duplicate entries %s and %s differ on field %s. (\"%s\" vs \"%s\")",
                               origkey, duplkey, fk, origentry.fields[fk], fval)
            


    def filter_bibolamazifile(self, bibolamazifile):
        #
        # bibdata is a pybtex.database.BibliographyData object
        #

        #if (not self.dupfile and not self.warn):
        #    logger.warning("duplicates filter: No action is being taken because neither "
        #                   "-sDupfile= nor -dWarn have been requested.")
        #    return

        bibdata = bibolamazifile.bibliographyData()

        used_citations = None
        
        if self.keep_only_used:
            jobname = self.jobname
            if not jobname:
                # use bibolamazi file base name
                jobname = re.sub(r'(\.bibolamazi)?\.bib$', '', os.path.basename(bibolamazifile.fname()))
                logger.debug("Using jobname %s guessed from bibolamazi file name", jobname)

            logger.debug("Getting list of used citations from %s.aux.", jobname)
            used_citations = auxfile.get_all_auxfile_citations(
                jobname, bibolamazifile, self.name(),
                self.jobname_search_dirs, return_set=True
            )

        duplicates = []

        arxivaccess = arxivutil.setup_and_get_arxiv_accessor(bibolamazifile)

        dupl_entryinfo_cache_accessor = self.cacheAccessor(DuplicatesEntryInfoCacheAccessor)

        for (key, entry) in iteritems(bibdata.entries):
            #cache_entries[key] = {}
            dupl_entryinfo_cache_accessor.prepare_entry_cache(key, entry, arxivaccess)


        newbibdata = BibliographyData()
        unused = BibliographyData()
        #unused_respawned = set() # because del unused.entries[key] is not implemented ... :(

        def copy_entry(entry):
            #return copy.deepcopy(entry) # too deep ...
            newpers = {}
            for role, plist in iteritems(entry.persons):
                newpers[role] = [copy.deepcopy(p) for p in plist]
            return Entry(type_=entry.type,
                         fields=entry.fields.items(), # will create own Fielddict
                         persons=newpers,
                         collection=entry.collection
                         )

        # Strategy: go through the list of entries, and each time keeping it if it is new,
        # or updating the original and registering the alias if it is a duplicate.
        #
        # With only_used, the situation is a little trickier as we cannot just discard the
        # entries as they are filtered: indeed, they might be duplicates of a used entry,
        # with which one should merge the bib information.
        #
        # So the full algorithm does not immediately discard the unused keys, but rather
        # keeps them in an `unused` list. If they are later required, they are respawned
        # into the actual new list.
        #

        def iter_over_bibdata(bibdata):
            # Iterate while respecting the order of the elements in the bibdata.
            # This might not be guaranteed with .items() or iteritems() (?)
            for k in bibdata.entries:
                yield k, bibdata.entries[k]
            #

        if self.ensure_conflict_keys_are_duplicates:
            # first, go through the whole bibliography, and make sure that any
            # entry of the form 'xxxxx.conflictkey.N' is a duplicate of 'xxxx'
            rx_conflictkey = re.compile(r'^(?P<origkey>.*)\.conflictkey\.\d+$', flags=re.IGNORECASE)
            confldup_entries_to_remove = []
            for (key, entry) in iter_over_bibdata(bibdata):
                m = rx_conflictkey.match(key)
                if not m: # not a conflictkey entry
                    continue

                origkey = m.group('origkey')

                logger.longdebug("Looking at conflictkey entry %s", key)

                if origkey not in bibdata.entries:
                    logger.warning("Entry with conflict key %s has no corresponding entry %s", key, origkey)
                    continue

                origentry = bibdata.entries[origkey]

                same, reason = self.compare_entries(entry, origentry,
                                                    dupl_entryinfo_cache_accessor.get_entry_cache(key),
                                                    dupl_entryinfo_cache_accessor.get_entry_cache(origkey))
                if not same:
                    logger.warning("Entry with conflict key %s is NOT a duplicate of entry %s: %s",
                                   key, origkey, reason)

                # entries are proper duplicates as expected, remove the conflictkey entry
                logger.debug("Removing conflictkey entry %s as it is really a duplicate of %s", key, origkey)

                # do merge the entries, in case they are not exact copies and
                # one has more information than the other
                self.update_entry_with_duplicate(origkey, origentry, key, entry)

                # don't delete the entry here, because it will mess up the for loop iteration!
                #del bibdata.entries[key]
                confldup_entries_to_remove.append(key)
            for k in confldup_entries_to_remove:
                del bibdata.entries[k]
                    
        if self.merge_duplicates or self.warn:

            for (key, entry) in iter_over_bibdata(bibdata):
                #
                # search the newbibdata object, in case this entry already exists.
                #
                #logger.longdebug('inspecting new entry %s ...', key)
                is_duplicate_of = None
                duplicate_original_is_unused = False
                for (nkey, nentry) in iteritems(newbibdata.entries):
                    same, reason = self.compare_entries(entry, nentry,
                                                        dupl_entryinfo_cache_accessor.get_entry_cache(key),
                                                        dupl_entryinfo_cache_accessor.get_entry_cache(nkey))
                    if same:
                        logger.debug("Entry %s is duplicate of existing entry %s: %s", key, nkey, reason)
                        is_duplicate_of = nkey
                        break
                for (nkey, nentry) in iteritems(unused.entries):
                    same, reason = self.compare_entries(entry, nentry,
                                                        dupl_entryinfo_cache_accessor.get_entry_cache(key),
                                                        dupl_entryinfo_cache_accessor.get_entry_cache(nkey))
                    if same:
                        logger.debug("Entry %s is duplicate of entry %s: %s", key, nkey, reason)
                        is_duplicate_of = nkey
                        duplicate_original_is_unused = True
                        break

                #
                # if it's a duplicate
                #
                if is_duplicate_of is not None:
                    dup = (key, is_duplicate_of)
                    if duplicate_original_is_unused:
                        self.update_entry_with_duplicate(is_duplicate_of, unused.entries[is_duplicate_of],
                                                         key, entry)
                    else:
                        # a duplicate of a key we have used. So update the original ...
                        self.update_entry_with_duplicate(is_duplicate_of, newbibdata.entries[is_duplicate_of],
                                                         key, entry)
                        # ... and register the alias.
                        duplicates.append(dup)

                    if duplicate_original_is_unused and used_citations and key in used_citations:
                        # if we had set the original in the unused list, but we need the
                        # alias, then respawn the original to the newbibdata so we can refer
                        # to it. Bonus: use the name with which we have referred to it, so we
                        # don't need to register any duplicate.
                        newbibdata.add_entry(key, unused.entries[is_duplicate_of])
                        #unused_respawned.add(is_duplicate_of)
                        del unused.entries[is_duplicate_of]
                else:
                    if used_citations is not None and key not in used_citations:
                        # new entry, but we don't want it. So add it to the unused list.
                        unused.add_entry(key, entry)
                    else:
                        # new entry and we want it. So add it to the main newbibdata list.
                        newbibdata.add_entry(key, entry)

            # output duplicates to the duplicates file

            if (self.dupfile):
                # and write definitions to the dupfile
                dupfilepath = os.path.join(bibolamazifile.fdir(), self.dupfile)
                check_overwrite_dupfile(dupfilepath)
                dupstrlist = []

                with codecs.open(dupfilepath, 'w', 'utf-8') as dupf:

                    dupf.write(BIBALIAS_HEADER.replace('####DUP_FILE_NAME####', self.dupfile))

                    if not self.custom_bibalias:
                        dupf.write(BIBALIAS_LATEX_DEFINITIONS)

                    # Note: Sort entries in some way (e.g. alphabetically according to
                    # (alias, original)), to avoid diffs in VCS's
                    for (dupalias, duporiginal) in sorted(duplicates, key=lambda x: (x[0],x[1])):
                        dupf.write((r'\bibalias{%s}{%s}' % (dupalias, duporiginal)) + "\n")
                        dupstrlist.append("\t%s is an alias of %s" % (dupalias,duporiginal)) 

                    dupf.write('\n\n')

                # issue debug message
                logger.debug("wrote duplicates to file: \n" + "\n".join(dupstrlist))

            if (self.warn and duplicates):
                def warnline(dupalias, duporiginal):
                    def fmt(key, entry, cache_entry):
                        s = ", ".join(string.capwords('%s, %s' % (x[0], "".join(x[1])))
                                      for x in cache_entry['pers'])
                        if 'title_clean' in cache_entry and cache_entry['title_clean']:
                            s += ', "' + (cache_entry['title_clean']).capitalize() + '"'
                        if 'j_abbrev' in cache_entry and cache_entry['j_abbrev']:
                            s += ', ' + cache_entry['j_abbrev']

                        f = entry.fields
                        if f.get('month',None) and f.get('year',None):
                            s += ', ' + f['month'] + ' ' + f['year']
                        elif f.get('month', None):
                            s += ', ' + f['month'] + ' <unknown year>'
                        elif f.get('year', None):
                            s += ', ' + f['year']

                        if 'doi' in entry.fields and entry.fields['doi']:
                            s += ', doi:'+entry.fields['doi']
                        if 'arxivinfo' in cache_entry and cache_entry['arxivinfo']:
                            s += ', arXiv:'+cache_entry['arxivinfo']['arxivid']
                        if 'note_cleaned' in cache_entry and cache_entry['note_cleaned']:
                            s += '; ' + cache_entry['note_cleaned']

                        return s

                    tw = textwrap.TextWrapper(width=DUPL_WARN_ENTRY_COLWIDTH)

                    fmtalias = fmt(dupalias, bibdata.entries[dupalias],
                                   dupl_entryinfo_cache_accessor.get_entry_cache(dupalias))
                    fmtorig = fmt(duporiginal, bibdata.entries[duporiginal],
                                  dupl_entryinfo_cache_accessor.get_entry_cache(duporiginal))
                    linesalias = tw.wrap(fmtalias)
                    linesorig = tw.wrap(fmtorig)
                    maxlines = max(len(linesalias), len(linesorig))
                    return (DUPL_WARN_ENTRY % { 'alias': dupalias,
                                                'orig': duporiginal
                                                }
                            + "\n".join( ('%s%s%s%s' %(' '*DUPL_WARN_ENTRY_BEGCOL,
                                                       linealias +
                                                       ' '*(DUPL_WARN_ENTRY_COLWIDTH-len(linealias)),
                                                       ' '*DUPL_WARN_ENTRY_COLSEP,
                                                       lineorig)
                                          for (linealias, lineorig) in
                                          zip(linesalias + ['']*(maxlines-len(linesalias)),
                                              linesorig + ['']*(maxlines-len(linesorig)))) )
                            + "\n\n"
                            )
                logger.warning(DUPL_WARN_TOP  +
                               "".join([ warnline(dupalias, duporiginal)
                                         for (dupalias, duporiginal) in duplicates
                                         ])  +
                               DUPL_WARN_BOTTOM % {'num_dupl': len(duplicates)})

            if self.merge_duplicates:
                #
                # set the new bibdata, without the duplicates --- DON'T DO THIS,
                # BECAUSE CACHES MAY HAVE KEPT A POINTER TO THE BIBDATA::
                #bibolamazifile.setBibliographyData(newbibdata)
                #
                # Instead, update bibolamazifile's bibliographyData() object itself.
                #
                bibolamazifile.setEntries(iteritems(newbibdata.entries))

            # end if self.merge_duplicates or self.warn
        
        return


def bibolamazi_filter_class():
    return DuplicatesFilter





# utility: taken from http://hetland.org/coding/python/levenshtein.py
#
# used to detect small differences in names which can result from either typos, or
# alternative spellings (e.g. 'Konig' vs 'Koenig')
def levenshtein(a,b):
    "Calculates the Levenshtein distance between a and b."
    n, m = len(a), len(b)
    if n > m:
        # Make sure n <= m, to use O(min(n,m)) space
        a,b = b,a
        n,m = m,n
        
    current = range(n+1)
    for i in range(1,m+1):
        previous, current = current, [i]+[0]*n
        for j in range(1,n+1):
            add, delete = previous[j]+1, current[j-1]+1
            change = previous[j-1]
            if a[j-1] != b[i-1]:
                change = change + 1
            current[j] = min(add, delete, change)
            
    return current[n]




def check_overwrite_dupfile(dupfilepath):
    if (not os.path.exists(dupfilepath)):
        return
    # path item exists (but could be dir, etc.)
    if (not os.path.isfile(dupfilepath)):
        raise BibFilterError('duplicates', "Can't overwrite non-file path `%s'"% (dupfilepath))
    
    with codecs.open(dupfilepath, 'r') as f:
        head_content = u''
        for countline in range(10):
            head_content += f.readline()

    if BIBALIAS_WARNING_HEADER not in head_content:
        raise BibFilterError('duplicates', "File `%s' does not seem to have been generated by bibolamazi. Won't overwrite. Please remove the file manually." %(dupfilepath))
